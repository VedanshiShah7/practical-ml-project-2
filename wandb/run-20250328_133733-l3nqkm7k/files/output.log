sh: line 0: cd: t-PatchGNN/tPatchGNN: No such file or directory
sh: line 0: cd: t-PatchGNN/tPatchGNN: No such file or directory
sh: line 0: cd: main/t-PatchGNN/tPatchGNN: No such file or directory
python: can't open file '/Users/vedanshi/Documents/GitHub/practical-ml-project-2/main/tPatchGNN/tPatchGNN/run_sample.py': [Errno 2] No such file or directory
PID, device: 85725 cpu
Experiment ID: 48851
/opt/anaconda3/envs/test_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd
  warnings.warn(
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [187556.0, 114038.0, 143432.0, 114953.0, 131999.0, 189598.0, 155582.0, 152416.0, 139711.0, 157558.0, 101067.0, 137990.0, 116094.0, 127105.0, 139400.0, 176066.0, 191615.0, 174828.0, 146589.0, 184581.0]
Test record ids (last 20): [122130.0, 123900.0, 184729.0, 188063.0, 172653.0, 188876.0, 196706.0, 180544.0, 126813.0, 199562.0, 100195.0, 163984.0, 115235.0, 197875.0, 176250.0, 117208.0, 123361.0, 103359.0, 101708.0, 177531.0]
data_max: tensor([6.7000e+01, 5.3000e+01, 2.7500e+01, 1.5100e+02, 7.3000e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 2.6500e+01, 1.8000e+02, 4.6950e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 9.7000e+01, 7.7700e+01, 2.2300e+01,
        1.0000e+02, 4.6100e+01, 4.0000e+01, 1.3900e+02, 1.0000e+02, 1.0000e+02,
        1.8710e+03, 3.4400e+01, 8.4400e+00, 5.7250e+02, 1.6260e+02, 5.0000e+01,
        2.3100e+02, 2.9700e+01, 2.4700e+02, 1.0000e+01, 7.9700e+02, 1.5000e+02,
        2.5460e+04, 1.0800e+00, 1.0000e+01, 1.0000e+03, 5.0000e+00, 9.0000e+01,
        2.0000e+01, 2.0000e+00, 2.5000e+00, 6.0000e+01, 1.6667e+01, 4.8000e+03,
        2.7000e+01, 1.1000e+04, 5.0000e+01, 9.0000e+01, 2.5000e+03, 6.3300e+01,
        9.3056e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 3.5000e+01,
        1.5509e+01, 6.3000e+00, 5.4000e+03, 2.7000e+02, 1.0000e+02, 4.0000e+01,
        1.0000e+02, 6.5000e+02, 3.8500e+02, 1.6332e+01, 5.2000e+02, 3.6000e+02,
        3.0000e+01, 1.2546e+01, 2.5000e+01, 1.4286e+02, 1.0500e+03, 2.7000e+02,
        5.0000e+03, 5.0000e+02, 8.0000e+00, 4.5000e+02, 3.7500e+03, 4.0000e+01,
        1.0000e+02, 1.1000e+03, 1.5000e+03, 2.8500e+02, 2.7041e+03, 5.0000e+00,
        2.0949e+00, 4.0000e+02, 8.5000e+02, 1.5000e+03, 7.0000e+02, 1.1000e+03])
data_min: tensor([1.0000e+00, 2.0000e+00, 0.0000e+00, 5.6000e+01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.4000e+00, 7.4000e+01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.5000e-01, 0.0000e+00,
        0.0000e+00, 2.0000e-01, 0.0000e+00, 1.0000e+00, 0.0000e+00, 7.0000e+00,
        0.0000e+00, 1.5000e-02, 2.0000e+00, 0.0000e+00, 1.0000e+00, 1.3000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1667e-01, 8.3333e-01,
        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0000e+00,
        1.0000e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0000e+00,
        2.5000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])
time_max: tensor(2879.7668)
tPatchGNN(
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=9, bias=True)
  (Filter_Generators): Sequential(
    (0): Linear(in_features=11, out_features=63, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=63, out_features=63, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=63, out_features=693, bias=True)
  )
  (ADD_PE): PositionalEncoding()
  (transformer_encoder): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (nodevec_linear1): ModuleList(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
  (nodevec_linear2): ModuleList(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
  (nodevec_gate1): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=74, out_features=1, bias=True)
      (1): Tanh()
      (2): ReLU()
    )
  )
  (nodevec_gate2): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=74, out_features=1, bias=True)
      (1): Tanh()
      (2): ReLU()
    )
  )
  (gconv): ModuleList(
    (0): gcn(
      (nconv): nconv()
      (mlp): linear(
        (mlp): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (temporal_agg): Sequential(
    (0): Linear(in_features=192, out_features=64, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=74, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)
/Users/vedanshi/Documents/GitHub/practical-ml-project-2/main/tPatchGNN/tPatchGNN/run_samples.py
2025-03-28 13:40:11
run_samples.py
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=8.0, stride=8.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=1, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=64, te_dim=10, node_dim=10, gpu='0', npatch=3, device=device(type='cuda'), PID=65027, ndim=96)
n_train_batches: 14073
val_res: {'loss': 0.11314710974693298, 'mse': 0.11314710974693298, 'mae': 0.224842831492424, 'rmse': 0.3363734781742096, 'mape': 1.779535174369812}
test_res: {'loss': 0.1140071377158165, 'mse': 0.1140071377158165, 'mae': 0.22563119232654572, 'rmse': 0.3376494348049164, 'mape': 1.7344491481781006}
PID, device: 85831 cpu
Experiment ID: 48851
/opt/anaconda3/envs/test_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd
  warnings.warn(
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [187556.0, 114038.0, 143432.0, 114953.0, 131999.0, 189598.0, 155582.0, 152416.0, 139711.0, 157558.0, 101067.0, 137990.0, 116094.0, 127105.0, 139400.0, 176066.0, 191615.0, 174828.0, 146589.0, 184581.0]
Test record ids (last 20): [122130.0, 123900.0, 184729.0, 188063.0, 172653.0, 188876.0, 196706.0, 180544.0, 126813.0, 199562.0, 100195.0, 163984.0, 115235.0, 197875.0, 176250.0, 117208.0, 123361.0, 103359.0, 101708.0, 177531.0]
data_max: tensor([6.7000e+01, 5.3000e+01, 2.7500e+01, 1.5100e+02, 7.3000e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 2.6500e+01, 1.8000e+02, 4.6950e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 9.7000e+01, 7.7700e+01, 2.2300e+01,
        1.0000e+02, 4.6100e+01, 4.0000e+01, 1.3900e+02, 1.0000e+02, 1.0000e+02,
        1.8710e+03, 3.4400e+01, 8.4400e+00, 5.7250e+02, 1.6260e+02, 5.0000e+01,
        2.3100e+02, 2.9700e+01, 2.4700e+02, 1.0000e+01, 7.9700e+02, 1.5000e+02,
        2.5460e+04, 1.0800e+00, 1.0000e+01, 1.0000e+03, 5.0000e+00, 9.0000e+01,
        2.0000e+01, 2.0000e+00, 2.5000e+00, 6.0000e+01, 1.6667e+01, 4.8000e+03,
        2.7000e+01, 1.1000e+04, 5.0000e+01, 9.0000e+01, 2.5000e+03, 6.3300e+01,
        9.3056e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 3.5000e+01,
        1.5509e+01, 6.3000e+00, 5.4000e+03, 2.7000e+02, 1.0000e+02, 4.0000e+01,
        1.0000e+02, 6.5000e+02, 3.8500e+02, 1.6332e+01, 5.2000e+02, 3.6000e+02,
        3.0000e+01, 1.2546e+01, 2.5000e+01, 1.4286e+02, 1.0500e+03, 2.7000e+02,
        5.0000e+03, 5.0000e+02, 8.0000e+00, 4.5000e+02, 3.7500e+03, 4.0000e+01,
        1.0000e+02, 1.1000e+03, 1.5000e+03, 2.8500e+02, 2.7041e+03, 5.0000e+00,
        2.0949e+00, 4.0000e+02, 8.5000e+02, 1.5000e+03, 7.0000e+02, 1.1000e+03])
data_min: tensor([1.0000e+00, 2.0000e+00, 0.0000e+00, 5.6000e+01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.4000e+00, 7.4000e+01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.5000e-01, 0.0000e+00,
        0.0000e+00, 2.0000e-01, 0.0000e+00, 1.0000e+00, 0.0000e+00, 7.0000e+00,
        0.0000e+00, 1.5000e-02, 2.0000e+00, 0.0000e+00, 1.0000e+00, 1.3000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1667e-01, 8.3333e-01,
        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0000e+00,
        1.0000e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0000e+00,
        2.5000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])
time_max: tensor(2879.7668)
tPatchGNN(
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=9, bias=True)
  (Filter_Generators): Sequential(
    (0): Linear(in_features=11, out_features=63, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=63, out_features=63, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=63, out_features=693, bias=True)
  )
  (ADD_PE): PositionalEncoding()
  (transformer_encoder): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (nodevec_linear1): ModuleList(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
  (nodevec_linear2): ModuleList(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
  (nodevec_gate1): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=74, out_features=1, bias=True)
      (1): Tanh()
      (2): ReLU()
    )
  )
  (nodevec_gate2): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=74, out_features=1, bias=True)
      (1): Tanh()
      (2): ReLU()
    )
  )
  (gconv): ModuleList(
    (0): gcn(
      (nconv): nconv()
      (mlp): linear(
        (mlp): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (temporal_agg): Sequential(
    (0): Linear(in_features=192, out_features=64, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=74, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)
/Users/vedanshi/Documents/GitHub/practical-ml-project-2/main/tPatchGNN/tPatchGNN/run_samples.py
2025-03-28 13:42:32
run_samples.py
n_train_batches: 14073
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=8.0, stride=8.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=1, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=64, te_dim=10, node_dim=10, gpu='0', npatch=3, device=device(type='cuda'), PID=65027, ndim=96)
val_res: {'loss': 0.11314710974693298, 'mse': 0.11314710974693298, 'mae': 0.224842831492424, 'rmse': 0.3363734781742096, 'mape': 1.779535174369812}
test_res: {'loss': 0.1140071377158165, 'mse': 0.1140071377158165, 'mae': 0.22563119232654572, 'rmse': 0.3376494348049164, 'mape': 1.7344491481781006}
PID, device: 85890 cpu
Experiment ID: 48851
/opt/anaconda3/envs/test_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd
  warnings.warn(
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [187556.0, 114038.0, 143432.0, 114953.0, 131999.0, 189598.0, 155582.0, 152416.0, 139711.0, 157558.0, 101067.0, 137990.0, 116094.0, 127105.0, 139400.0, 176066.0, 191615.0, 174828.0, 146589.0, 184581.0]
Test record ids (last 20): [122130.0, 123900.0, 184729.0, 188063.0, 172653.0, 188876.0, 196706.0, 180544.0, 126813.0, 199562.0, 100195.0, 163984.0, 115235.0, 197875.0, 176250.0, 117208.0, 123361.0, 103359.0, 101708.0, 177531.0]
data_max: tensor([6.7000e+01, 5.3000e+01, 2.7500e+01, 1.5100e+02, 7.3000e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 2.6500e+01, 1.8000e+02, 4.6950e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 9.7000e+01, 7.7700e+01, 2.2300e+01,
        1.0000e+02, 4.6100e+01, 4.0000e+01, 1.3900e+02, 1.0000e+02, 1.0000e+02,
        1.8710e+03, 3.4400e+01, 8.4400e+00, 5.7250e+02, 1.6260e+02, 5.0000e+01,
        2.3100e+02, 2.9700e+01, 2.4700e+02, 1.0000e+01, 7.9700e+02, 1.5000e+02,
        2.5460e+04, 1.0800e+00, 1.0000e+01, 1.0000e+03, 5.0000e+00, 9.0000e+01,
        2.0000e+01, 2.0000e+00, 2.5000e+00, 6.0000e+01, 1.6667e+01, 4.8000e+03,
        2.7000e+01, 1.1000e+04, 5.0000e+01, 9.0000e+01, 2.5000e+03, 6.3300e+01,
        9.3056e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 3.5000e+01,
        1.5509e+01, 6.3000e+00, 5.4000e+03, 2.7000e+02, 1.0000e+02, 4.0000e+01,
        1.0000e+02, 6.5000e+02, 3.8500e+02, 1.6332e+01, 5.2000e+02, 3.6000e+02,
        3.0000e+01, 1.2546e+01, 2.5000e+01, 1.4286e+02, 1.0500e+03, 2.7000e+02,
        5.0000e+03, 5.0000e+02, 8.0000e+00, 4.5000e+02, 3.7500e+03, 4.0000e+01,
        1.0000e+02, 1.1000e+03, 1.5000e+03, 2.8500e+02, 2.7041e+03, 5.0000e+00,
        2.0949e+00, 4.0000e+02, 8.5000e+02, 1.5000e+03, 7.0000e+02, 1.1000e+03])
data_min: tensor([1.0000e+00, 2.0000e+00, 0.0000e+00, 5.6000e+01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 1.4000e+00, 7.4000e+01, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        4.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.5000e-01, 0.0000e+00,
        0.0000e+00, 2.0000e-01, 0.0000e+00, 1.0000e+00, 0.0000e+00, 7.0000e+00,
        0.0000e+00, 1.5000e-02, 2.0000e+00, 0.0000e+00, 1.0000e+00, 1.3000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1667e-01, 8.3333e-01,
        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0000e+00,
        1.0000e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0000e+00,
        2.5000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])
time_max: tensor(2879.7668)
tPatchGNN(
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=9, bias=True)
  (Filter_Generators): Sequential(
    (0): Linear(in_features=11, out_features=63, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=63, out_features=63, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=63, out_features=693, bias=True)
  )
  (ADD_PE): PositionalEncoding()
  (transformer_encoder): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (nodevec_linear1): ModuleList(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
  (nodevec_linear2): ModuleList(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
  (nodevec_gate1): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=74, out_features=1, bias=True)
      (1): Tanh()
      (2): ReLU()
    )
  )
  (nodevec_gate2): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=74, out_features=1, bias=True)
      (1): Tanh()
      (2): ReLU()
    )
  )
  (gconv): ModuleList(
    (0): gcn(
      (nconv): nconv()
      (mlp): linear(
        (mlp): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (temporal_agg): Sequential(
    (0): Linear(in_features=192, out_features=64, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=74, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
)
/Users/vedanshi/Documents/GitHub/practical-ml-project-2/main/tPatchGNN/tPatchGNN/run_samples.py
n_train_batches: 14073
2025-03-28 13:44:35
run_samples.py
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=8.0, stride=8.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=1, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=64, te_dim=10, node_dim=10, gpu='0', npatch=3, device=device(type='cuda'), PID=65027, ndim=96)
val_res: {'loss': 0.11314710974693298, 'mse': 0.11314710974693298, 'mae': 0.224842831492424, 'rmse': 0.3363734781742096, 'mape': 1.779535174369812}
test_res: {'loss': 0.1140071377158165, 'mse': 0.1140071377158165, 'mae': 0.22563119232654572, 'rmse': 0.3376494348049164, 'mape': 1.7344491481781006}
/opt/anaconda3/envs/test_env/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.
  warnings.warn(
Validation F1 Score: 0.9073
Predictions saved to results/predictions.csv
Chatbot Response: Elevated glucose levels, particularly hyperglycemia, are significantly associated with increased mortality risk across various patient populations and disease states.  This association is independent of pre-existing diabetes.  Higher glucose levels are linked to increased risk of infections, prolonged hospital stays, impaired wound healing, and organ dysfunction, all of which contribute to poorer outcomes and higher mortality.  Therefore, monitoring and managing glucose levels, especially in critically ill patients or those with acute illnesses, is crucial for improving patient outcomes and reducing mortality risk.  The specific threshold for concerning hyperglycemia varies depending on the clinical context and patient factors.
